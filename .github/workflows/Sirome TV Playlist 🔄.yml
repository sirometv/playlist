name: Combined Playlist Skipper 1üîÑ

on:
  schedule:
    - cron: '*/5 * * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update-playlists:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
        
    - name: Download all playlists
      run: |
        echo "üì• Downloading all required playlists..."
        
        # Source playlists
        curl -s -L -o source_playlist.m3u "https://raw.githubusercontent.com/sirometv/playlist/gh-pages/sirometv.m3u"
        curl -s -L -o ayna_source.m3u "https://raw.githubusercontent.com/sirometv/playlist/main/Ayna.m3u"
        curl -s -L -o fancode_source.m3u "https://raw.githubusercontent.com/abusaeeidx/Mrgify-BDIX-IPTV/main/playlist.m3u"
        
        # Verify downloads
        for file in source_playlist.m3u ayna_source.m3u fancode_source.m3u; do
          if [ ! -f "$file" ] || [ ! -s "$file" ]; then
            echo "‚ùå ERROR: $file download failed!"
            exit 1
          fi
          LINES=$(wc -l < "$file")
          echo "‚úÖ $file downloaded - $LINES lines"
        done
        
        # ‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨‡ßá ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶ï‡¶™‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
        cp source_playlist.m3u main_playlist.m3u
        echo "üìã Source playlist copied to main_playlist.m3u"
        
    - name: Backup original playlist
      run: |
        cp source_playlist.m3u main_playlist_backup.m3u
        echo "üì¶ Original playlist backed up"
        
    - name: Update All Channel üîÑ
      run: |
        echo "üîÑ Starting All Channel üîÑ update..."
        
        python3 - << 'EOF'
        import re
        from datetime import datetime
        import pytz
        import os
        
        # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç ‡¶°‡¶ø‡¶ï‡¶∂‡¶®‡¶æ‡¶∞‡¶ø
        CHANNEL_MAPPING = {
            # ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡¶ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤
            "T Sports": ["T Sports HD", "T Sports", "TSports"],
            "A Sports": ["A Sports"],
            "Channel 9": ["Channel 9"],
            "Desh TV": ["Desh TV"],
            "BTV National": ["BTV NATIONAL HD", "BTV World"],
            "BTV CTG": ["BTV CTG"],
            "Somoy TV": ["Somoy News TV", "Somoy TV"],
            "Jamuna TV": ["Jamuna TV","JAMUNA TV"],
            "Ekushey TV": ["Ekushey TV"],
            "NTV BD": ["NTV"],
            "NTV-Europe": ["NTV Europe"],
            "Channel 24": ["Channel 24"],
            "Deen TV": ["Deen TV"],
            "SATV": ["SA TV"],
            "GTV": ["GTV", "GTV HD", "Gazi TV"],
            "PTV Sports": ["PTV Sports"],
            "Channel I": ["Channel I"],
            "Bangla Vision": ["Bangla Vision"],
            "News 24": ["News 24 BD"], 
            "Boishakhi TV": ["Boishakhi TV"],
            "Bijoy TV": ["Bijoy TV"],
            "ATN Bangla": ["ATN Bangla"],
            "ATN-Bangla News": ["ATN News"],
            "DBC News": ["DBC News"],
            "Independent TV": ["Independent TV"],
            "Bangla TV": ["Bangla TV"],
            "RTV": ["RTV"],
            "My TV": ["My TV"],
            "Duronto": ["Duronto TV"],
            "Deepto TV": ["Deepto TV"],
            "Mohona TV": ["Mohona TV"],
            "Ananda TV": ["Ananda TV"],
            "Asian TV": ["Asian TV"],
            "Gaan Bangla": ["Gaan Bangla", "GAAN BANGLA", "gaan bangla", "Gaan", "GAAN"],
            "Channel 5 TV": ["Channel 5 TV"],
            "Music Bangla": ["Music Bangla"],
            "Ekattor TV": ["Ekattor TV"],
            "Nagorik TV": ["Nagorik TV"],
            "Ekhon TV": ["Ekhon TV"],
            "Peace TV Bangla": ["Peace TV Bangla HD"],
            "Islam Bangla": ["Islam Bangla"],
            "Iqra TV ": ["Iqra Bangla"],
            "Madani TV": ["Madani TV"],
            "News21 TV": ["News21 TV"],
            "Bangla 21": ["Bangla 21"],
            "Movie Bangla": ["Movie Bangla"],
            "Matri Bhumi": ["Matri Bhumi"],
            "Magic Bangla": ["Magic Bangla"],
            "Alpona TV": ["Alpona TV"],
            "Channel A1": ["Channel A1"],
            "Shadin Bangla": ["Shadin Bangla"],
            "Channel S UK": ["Channel S UK HD"],
            "Nandan TV": ["Nandan TV"],
            "Global TV": ["Global TV"],
            "Deshi TV": ["Deshi TV", "Deshi TV (720p)"],
            "Falguni TV": ["Falguni TV"],
            "Channel S BD": ["Channel S BD","Channel S HD"],
            "Rajdhani TV": ["Rajdhani TV"],
            "Rongeen TV": ["Rongeen TV"],
            "Sonic Bangla": ["Sonic Bangla"],
            "Discovery Kids": ["Discovery Kids"],
            "NAN TV": ["NAN TV"],
            
            # ‡¶Ü‡¶®‡ßç‡¶§‡¶∞‡ßç‡¶ú‡¶æ‡¶§‡¶ø‡¶ï ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤
            "Star Jalsha": ["Star Jalsha HD", "Star Jalsha"],
            "Star-Jalsha Movies": ["Star Jalsha Movies HD"],
            "Zee Bangla": ["Zee Bangla International", "Zee Bangla"],
            "Zee-Bangla Cinema": ["Zee Bangla Cinema"],
            "Colors Bengali": ["colors bangla"],
            "Sangeet Bangla": ["Sangeet Bangla"],
            "Sony Aaat": ["Sony Aaat"],
            "Enter 10 Bangla": ["Enter 10 Bangla"],
            "Zee 24 Ghanta": ["Zee 24 Ghanta"],
        }
        
        def get_bangladesh_time():
            """‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡ßá"""
            bangladesh_tz = pytz.timezone('Asia/Dhaka')
            now = datetime.now(bangladesh_tz)
            return now.strftime("%Y-%m-%d %I:%M:%S%p BD Time")
        
        def extract_channel_from_source(source_file, channel_name):
            """‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá"""
            with open(source_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶¨‡ßç‡¶≤‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
            pattern = r'(#EXTINF:-1[^\n]*{}[^\n]*)\n((?:http[^\n]*\n)+)'.format(re.escape(channel_name))
            match = re.search(pattern, content, re.IGNORECASE)
            
            if not match:
                return None
            
            extinf_line = match.group(1)
            urls_block = match.group(2)
            
            # URL ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡¶æ
            urls = [url.strip() for url in urls_block.strip().split('\n') if url.strip()]
            
            # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶§‡¶•‡ßç‡¶Ø
            logo_match = re.search(r'tvg-logo="([^"]*)"', extinf_line)
            group_match = re.search(r'group-title="([^"]*)"', extinf_line)
            
            return {
                'extinf_line': extinf_line,
                'urls': urls,
                'tvg_logo': logo_match.group(1) if logo_match else "",
                'group_title': group_match.group(1) if group_match else "",
                'full_entry': match.group(0).strip()
            }
        
        def extract_channels_from_ayna(ayna_file, search_keywords):
            """Ayna ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°‡ßá‡¶∞ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá"""
            all_urls = []
            
            with open(ayna_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶ñ‡ßã‡¶Å‡¶ú‡¶æ
            for keyword in search_keywords:
                pattern = r'#EXTINF:[^\n]*{}[^\n]*\n((?:http[^\n]*\n?)+)'.format(re.escape(keyword))
                matches = re.finditer(pattern, content, re.IGNORECASE)
                
                for match in matches:
                    urls_block = match.group(1)
                    urls = [url.strip() for url in urls_block.strip().split('\n') if url.strip()]
                    all_urls.extend(urls)
            
            # ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠ ‡¶ï‡¶∞‡ßá ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß´‡¶ü‡¶ø URL ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶®
            unique_urls = []
            seen = set()
            for url in all_urls:
                if url not in seen and len(unique_urls) < 5:
                    seen.add(url)
                    unique_urls.append(url)
            
            return unique_urls[:5]  # ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß´‡¶ü‡¶ø URL
        
        def update_channel_entry(main_content, channel_data, ayna_urls):
            """‡¶è‡¶ï‡¶ü‡¶ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶è‡¶®‡ßç‡¶ü‡ßç‡¶∞‡¶ø ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßá"""
            channel_name = channel_data['channel_name']
            source_urls = channel_data['urls']
            
            timestamp = get_bangladesh_time()
            
            # ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ URL ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ
            matched_urls = []
            unmatched_source_urls = []
            
            for url in source_urls:
                if url in ayna_urls:
                    matched_urls.append(url)
                else:
                    unmatched_source_urls.append(url)
            
            # Ayna ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡¶§‡ßÅ‡¶® URL ‡¶ó‡ßÅ‡¶≤‡ßã (‡¶Ø‡ßá‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∏‡ßã‡¶∞‡ßç‡¶∏‡ßá ‡¶®‡ßá‡¶á)
            new_ayna_urls = [url for url in ayna_urls if url not in source_urls]
            
            # ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß´‡¶ü‡¶ø URL ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∏‡¶Æ‡¶®‡ßç‡¶¨‡ßü ‡¶ï‡¶∞‡¶æ
            all_urls = []
            
            # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ URL ‡¶ó‡ßÅ‡¶≤‡ßã (‡¶∏‡ßç‡¶ï‡¶ø‡¶™‡¶°)
            all_urls.extend(matched_urls)
            
            # ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶®‡¶§‡ßÅ‡¶® Ayna URL ‡¶ó‡ßÅ‡¶≤‡ßã (‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß´‡¶ü‡¶ø ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§)
            available_slots = 5 - len(all_urls)
            if available_slots > 0 and new_ayna_urls:
                all_urls.extend(new_ayna_urls[:available_slots])
            
            # ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶¨‡¶æ‡¶ï‡¶ø ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ URL ‡¶ó‡ßÅ‡¶≤‡ßã
            available_slots = 5 - len(all_urls)
            if available_slots > 0 and unmatched_source_urls:
                all_urls.extend(unmatched_source_urls[:available_slots])
            
            # ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß´‡¶ü‡¶ø URL ‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶ï‡¶∞‡¶æ
            all_urls = all_urls[:5]
            
            # URL ‡¶¨‡ßç‡¶≤‡¶ï ‡¶§‡ßà‡¶∞‡¶ø
            url_block = '\n'.join(all_urls) + '\n'
            
            # ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶Æ‡ßá‡¶∏‡ßá‡¶ú ‡¶§‡ßà‡¶∞‡¶ø
            if matched_urls and new_ayna_urls:
                # ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶Æ‡¶ø‡¶≤‡ßá‡¶õ‡ßá, ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶®‡¶§‡ßÅ‡¶® ‡¶Ø‡ßã‡¶ó ‡¶π‡ßü‡ßá‡¶õ‡ßá
                update_message = f"#AUTO-UPDATED - {channel_name} ‚õî SKIPPED üü¢ Other Link üü¢ - {timestamp} üü° - üîÑ - üî¥\n"
            elif matched_urls and not new_ayna_urls:
                # ‡¶∏‡¶¨‡¶á ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶ó‡ßá‡¶õ‡ßá, ‡¶®‡¶§‡ßÅ‡¶® ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶®‡ßá‡¶á
                update_message = f"#AUTO-UPDATED - {channel_name} ‚õî SKIPPED ‚õî - {timestamp} üü° - üîÑ - üî¥\n"
            else:
                # ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á ‡¶Æ‡ßá‡¶≤‡ßá‡¶®‡¶ø, ‡¶∏‡¶¨ ‡¶®‡¶§‡ßÅ‡¶®
                update_message = f"#AUTO-UPDATED - {channel_name} ‚úÖ- {timestamp} üü° - üîÑ - üî¥\n"
            
            # ‡¶™‡ßÅ‡¶∞‡¶æ‡¶§‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶¨‡ßç‡¶≤‡¶ï ‡¶∞‡¶ø‡¶™‡ßç‡¶≤‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ
            old_pattern = r'({})\n((?:http[^\n]*\n)+)'.format(re.escape(channel_data['extinf_line']))
            
            def replacement(match):
                return match.group(1) + '\n' + update_message + url_block
            
            updated_content = re.sub(old_pattern, replacement, main_content, flags=re.MULTILINE)
            
            return updated_content, len(matched_urls), len(new_ayna_urls), len(all_urls)
        
        # ‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('main_playlist.m3u', 'r', encoding='utf-8') as f:
            main_content = f.read()
        
        # Check if manual run or scheduled
        is_manual_run = os.environ.get('GITHUB_EVENT_NAME') == 'workflow_dispatch'
        
        updated_content = main_content
        total_channels_processed = 0
        total_skipped = 0
        total_partial = 0
        total_updated = 0
        
        print(f"üéØ Processing {len(CHANNEL_MAPPING)} channels from mapping dictionary")
        
        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
        for main_channel, search_keywords in CHANNEL_MAPPING.items():
            print(f"\nüîç Processing: {main_channel}")
            print(f"   Search keywords: {search_keywords}")
            
            # ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶§‡¶•‡ßç‡¶Ø ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π
            channel_data = extract_channel_from_source('source_playlist.m3u', main_channel)
            
            if not channel_data:
                print(f"   ‚ö†Ô∏è Channel '{main_channel}' not found in source playlist, skipping...")
                continue
            
            channel_data['channel_name'] = main_channel
            total_channels_processed += 1
            
            # Ayna ‡¶•‡ßá‡¶ï‡ßá ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π (‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡ß´‡¶ü‡¶ø)
            ayna_urls = extract_channels_from_ayna('ayna_source.m3u', search_keywords)
            
            if ayna_urls:
                print(f"   Found {len(ayna_urls)} URLs from Ayna")
            else:
                print(f"   No URLs found in Ayna for keywords: {search_keywords}")
            
            # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
            updated_content, matched_count, new_count, total_urls = update_channel_entry(
                updated_content, 
                channel_data, 
                ayna_urls
            )
            
            # ‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∏ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü
            if matched_count > 0 and new_count == 0:
                total_skipped += 1
                print(f"   ‚õî SKIPPED - {matched_count} URLs matched, 0 new added")
            elif matched_count > 0 and new_count > 0:
                total_partial += 1
                print(f"   üü° PARTIAL - {matched_count} matched, {new_count} new URLs added (total: {total_urls})")
            elif new_count > 0:
                total_updated += 1
                print(f"   ‚úÖ UPDATED - {new_count} new URLs added (total: {total_urls})")
            else:
                print(f"   ‚ÑπÔ∏è NO CHANGE - No updates needed")
        
        # ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü temporary ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('temp_after_all_channel.m3u', 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        print(f"\nüéâ All Channel üîÑ Update Summary:")
        print(f"   Run type: {'Manual' if is_manual_run else 'Scheduled'}")
        print(f"   Total channels in mapping: {len(CHANNEL_MAPPING)}")
        print(f"   Total channels processed: {total_channels_processed}")
        print(f"   ‚úÖ Fully updated channels: {total_updated}")
        print(f"   üü° Partially updated channels: {total_partial}")
        print(f"   ‚õî Skipped channels: {total_skipped}")
        
        # ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶æ‡¶¶ ‡¶™‡ßú‡¶æ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤
        remaining = len(CHANNEL_MAPPING) - total_channels_processed
        if remaining > 0:
            print(f"   ‚ö†Ô∏è Channels not found in source: {remaining}")
        
        EOF
        
        echo "‚úÖ All Channel üîÑ update completed!"
        
    - name: Update Live Fancode üîÑ
      run: |
        echo "üîÑ Starting Live Fancode üîÑ update..."
        
        python3 - << 'EOF'
        import re
        from datetime import datetime
        import pytz
        
        # ‚öôÔ∏è CONFIGURATION
        TARGET_LINE_NUMBER = 25  # ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶á‡¶ö‡ßç‡¶õ‡¶æ‡¶Æ‡¶§ ‡¶≤‡¶æ‡¶á‡¶® ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®
        
        # ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤‡ßá‡¶∞ ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü
        TARGET_GROUP_TITLES = [
            "Live Event",
            # "Cricket",    # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Akash Go",    # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Football",    # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Sports",      # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Movies",      # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
        ]
        
        def get_bangladesh_time():
            """‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡ßá"""
            bangladesh_tz = pytz.timezone('Asia/Dhaka')
            now = datetime.now(bangladesh_tz)
            return now.strftime("%Y-%m-%d %I:%M:%S%p BD Time")
        
        def get_channel_name_strategy(group_title, extinf_line):
            """‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ‡ßá‡¶∞ strategy ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡ßá"""
            # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ extract ‡¶ï‡¶∞‡¶æ
            channel_name_match = re.search(r',([^,\n]+)$', extinf_line)
            original_name = channel_name_match.group(1).strip() if channel_name_match else ""
            
            if group_title == "Live Event":
                return "üü¢Live Matchüî¥"  # Live Event ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü ‡¶®‡¶æ‡¶Æ
            elif group_title == "Sports":
                return "Sports Live"  # Sports ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü ‡¶®‡¶æ‡¶Æ
            elif group_title == "Akash Go":
                return original_name  # Akash Go ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶∏‡¶≤ ‡¶®‡¶æ‡¶Æ ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá
            else:
                return original_name  # ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶∏‡¶≤ ‡¶®‡¶æ‡¶Æ
        
        def extract_group_channels(source_file, group_title):
            """‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ extract ‡¶ï‡¶∞‡ßá"""
            channels = []
            
            with open(source_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
            pattern = r'(#EXTINF:-1[^\n]*group-title="[^"]*{}[^"]*"[^\n]*\n)(http[^\n]+)'.format(re.escape(group_title))
            
            matches = re.finditer(pattern, content, re.IGNORECASE)
            
            for match in matches:
                extinf_line = match.group(1).strip()
                url_line = match.group(2).strip()
                
                # tvg-logo extract ‡¶ï‡¶∞‡¶æ
                logo_match = re.search(r'tvg-logo="([^"]*)"', extinf_line)
                tvg_logo = logo_match.group(1) if logo_match else "https://i.postimg.cc/d1hHy4ss/11.png"
                
                # ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£
                channel_name = get_channel_name_strategy(group_title, extinf_line)
                
                channels.append({
                    'extinf_line': extinf_line,
                    'url_line': url_line,
                    'tvg_logo': tvg_logo,
                    'channel_name': channel_name,
                    'group_title': group_title
                })
            
            print(f"‚úÖ Found {len(channels)} channels for group '{group_title}'")
            return channels
        
        def force_groups_at_line(main_content, all_group_channels, target_line):
            """‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶∏‡¶¨ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶∏‡ßá‡¶ï‡¶∂‡¶® ‡¶∞‡¶æ‡¶ñ‡ßá"""
            timestamp = get_bangladesh_time()
            
            # ‡¶∏‡¶¨ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø combined section ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
            new_combined_section = f"#AUTO-UPDATED - Multiple Groups - {timestamp}\n"
            
            total_channels = 0
            has_any_channels = False
            
            for group_title, channels in all_group_channels.items():
                if channels and len(channels) > 0:  # ‡¶∂‡ßÅ‡¶ß‡ßÅ channels ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶è‡¶¨‡¶Ç empty ‡¶®‡¶æ ‡¶π‡¶≤‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá
                    new_combined_section += f"# --- {group_title} Group ---\n"
                    
                    for channel in channels:
                        # ‡¶®‡¶§‡ßÅ‡¶® EXTINF ‡¶≤‡¶æ‡¶á‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ - ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ
                        new_extinf_line = f'#EXTINF:-1 tvg-logo="{channel["tvg_logo"]}" group-title="{group_title}", {channel["channel_name"]}\n'
                        new_combined_section += new_extinf_line
                        new_combined_section += channel["url_line"] + "\n"
                    
                    total_channels += len(channels)
                    has_any_channels = True
                    print(f"üì∫ Added {len(channels)} channels for {group_title}")
            
            # No channels found ‡¶≤‡¶æ‡¶á‡¶® ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá ‡¶®‡¶æ
            if not has_any_channels:
                print("‚ÑπÔ∏è No channels found for any active group - creating empty AUTO-UPDATED section")
            
            # ‡¶Æ‡ßá‡¶á‡¶® ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü‡¶ï‡ßá ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ
            lines = main_content.split('\n')
            
            # TARGET_LINE_NUMBER validation
            if target_line < 1:
                target_line = 1
            if target_line > len(lines):
                target_line = len(lines)
            
            print(f"üìç Target line: {target_line} (Total lines: {len(lines)})")
            
            # existing ANY AUTO-UPDATED section ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶∞‡¶æ‡¶®‡ßã (‡¶™‡ßÅ‡¶∞‡¶æ‡¶®‡ßã ‡¶∏‡¶¨ versions)
            auto_updated_start = -1
            auto_updated_end = -1
            
            # existing section ‡¶è‡¶∞ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶è‡¶¨‡¶Ç ‡¶∂‡ßá‡¶∑ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ - ANY AUTO-UPDATED section
            for i, line in enumerate(lines):
                if '#AUTO-UPDATED' in line and 'Multiple Groups' in line:
                    auto_updated_start = i
                    print(f"üîç Found existing AUTO-UPDATED section at line {i+1}")
                    # section ‡¶è‡¶∞ ‡¶∂‡ßá‡¶∑ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ
                    for j in range(i + 1, len(lines)):
                        if not (lines[j].startswith('#EXTINF') or lines[j].startswith('http') or lines[j].startswith('# ---') or lines[j].strip() == ''):
                            auto_updated_end = j
                            break
                    if auto_updated_end == -1:
                        auto_updated_end = len(lines)
                    break
            
            if auto_updated_start != -1:
                print(f"üóëÔ∏è Removing existing AUTO-UPDATED section (lines {auto_updated_start+1}-{auto_updated_end})")
                # existing section ‡¶∏‡¶∞‡¶æ‡¶®‡ßã
                lines = lines[:auto_updated_start] + lines[auto_updated_end:]
            
            # ‡¶®‡¶§‡ßÅ‡¶® section ‡¶ü‡¶æ‡¶∞‡ßç‡¶ó‡ßá‡¶ü ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ
            insert_position = target_line - 1  # 0-based index
            
            # ‡¶Ø‡¶¶‡¶ø insert position existing removal ‡¶è‡¶∞ ‡¶™‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶π‡¶Ø‡¶º‡ßá ‡¶•‡¶æ‡¶ï‡ßá, adjust ‡¶ï‡¶∞‡¶æ
            if auto_updated_start != -1 and auto_updated_start < insert_position:
                insert_position -= (auto_updated_end - auto_updated_start)
            
            if insert_position > len(lines):
                insert_position = len(lines)
            
            new_lines = lines[:insert_position] + new_combined_section.split('\n') + lines[insert_position:]
            updated_content = '\n'.new_lines
            
            print(f"‚úÖ Multiple Groups section positioned at line {target_line}")
            
            return updated_content, total_channels
        
        # All Channel update ‡¶è‡¶∞ ‡¶™‡¶∞‡ßá‡¶∞ temporary ‡¶´‡¶æ‡¶á‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('temp_after_all_channel.m3u', 'r', encoding='utf-8') as f:
            main_content = f.read()
        
        # Check if manual run or scheduled
        import os
        is_manual_run = os.environ.get('GITHUB_EVENT_NAME') == 'workflow_dispatch'
        
        # Filter out commented groups (lines starting with #)
        active_groups = [group for group in TARGET_GROUP_TITLES if not group.startswith('#')]
        
        if not active_groups:
            print("‚ùå ERROR: No active groups found! Please uncomment at least one group in TARGET_GROUP_TITLES")
            print("Current TARGET_GROUP_TITLES:", TARGET_GROUP_TITLES)
            exit(1)
        
        print(f"üéØ Active groups: {active_groups}")
        
        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø active ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ extract ‡¶ï‡¶∞‡ßÅ‡¶®
        all_group_channels = {}
        total_found_channels = 0
        has_any_channels_found = False
        
        for group_title in active_groups:
            print(f"\nüîç Processing group: {group_title}")
            channels = extract_group_channels('fancode_source.m3u', group_title)
            # ‡¶∂‡ßÅ‡¶ß‡ßÅ channels ‡¶•‡¶æ‡¶ï‡¶≤‡ßá all_group_channels ‡¶è ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá
            if channels:
                all_group_channels[group_title] = channels
                total_found_channels += len(channels)
                has_any_channels_found = True
            else:
                print(f"‚ÑπÔ∏è No channels found for group '{group_title}', skipping...")
        
        # ‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶® - ‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º TARGET_LINE_NUMBER ‡¶è ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá
        updated_content, total_added_channels = force_groups_at_line(main_content, all_group_channels, TARGET_LINE_NUMBER)
        
        # ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶´‡¶æ‡¶á‡¶®‡¶æ‡¶≤ ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('sirometv_updated.m3u', 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        print(f"\nüéâ Live Fancode üîÑ Update Summary:")
        print(f"   Run type: {'Manual' if is_manual_run else 'Scheduled'}")
        print(f"   Target line: {TARGET_LINE_NUMBER}")
        print(f"   Total active groups: {len(active_groups)}")
        print(f"   Total channels found: {total_found_channels}")
        print(f"   Total channels added: {total_added_channels}")
        
        EOF
        
        echo "‚úÖ Live Fancode üîÑ update completed!"
        
    - name: Verify combined updates
      run: |
        echo "üîç Verifying combined playlist updates..."
        
        if [ ! -f "sirometv_updated.m3u" ] || [ ! -s "sirometv_updated.m3u" ]; then
          echo "‚ùå ERROR: Updated playlist is missing or empty!"
          exit 1
        fi
        
        # Compare with backup
        OLD_LINES=$(wc -l < main_playlist_backup.m3u)
        NEW_LINES=$(wc -l < sirometv_updated.m3u)
        DIFF=$((NEW_LINES - OLD_LINES))
        
        echo "üìä Line count comparison:"
        echo "   Original: $OLD_LINES lines"
        echo "   Updated: $NEW_LINES lines"
        echo "   Difference: $DIFF lines"
        
        # Check All Channel updates
        echo "üîç Checking All Channel üîÑ updates:"
        ALL_CHANNEL_UPDATES=$(grep -c "#AUTO-UPDATED -.*‚úÖ-" sirometv_updated.m3u || echo "0")
        ALL_CHANNEL_SKIPS=$(grep -c "#AUTO-UPDATED -.*‚õî SKIPPED ‚õî" sirometv_updated.m3u || echo "0")
        ALL_CHANNEL_PARTIAL=$(grep -c "#AUTO-UPDATED -.*‚õî SKIPPED üü¢ Other Link üü¢" sirometv_updated.m3u || echo "0")
        echo "   ‚úÖ All Channel updates: $ALL_CHANNEL_UPDATES channels"
        echo "   ‚õî All Channel skips: $ALL_CHANNEL_SKIPS channels"
        echo "   üü° All Channel partial updates: $ALL_CHANNEL_PARTIAL channels"
        
        # Check maximum 5 URLs per channel
        echo "üîç Checking maximum 5 URLs per channel:"
        BAD_CHANNELS=$(python3 -c "
import re
with open('sirometv_updated.m3u', 'r') as f:
    content = f.read()

# Find all AUTO-UPDATED sections
pattern = r'#AUTO-UPDATED[^\n]*\n((?:http[^\n]*\n)+)'
matches = re.findall(pattern, content)

bad_count = 0
for i, urls_block in enumerate(matches):
    urls = [url.strip() for url in urls_block.strip().split('\n') if url.strip()]
    if len(urls) > 5:
        bad_count += 1
        print(f'Channel with {len(urls)} URLs (max 5 allowed)')

print(f'Total channels with >5 URLs: {bad_count}')
if bad_count > 0:
    exit(1)
")
        
        # Check Live Fancode updates
        echo "üîç Checking Live Fancode üîÑ updates:"
        MULTIPLE_GROUPS_SECTION=$(grep -c "#AUTO-UPDATED - Multiple Groups" sirometv_updated.m3u || echo "0")
        echo "   Multiple Groups section: $MULTIPLE_GROUPS_SECTION"
        
        if [ "$MULTIPLE_GROUPS_SECTION" -eq "0" ]; then
            echo "‚ö†Ô∏è Warning: No Multiple Groups section found"
        else
            # Check section position
            SECTION_LINE=$(grep -n "#AUTO-UPDATED - Multiple Groups" sirometv_updated.m3u | cut -d: -f1)
            echo "   üìç Multiple Groups section at line: $SECTION_LINE"
        fi
        
        # Check for Live Event group
        LIVE_EVENT_COUNT=$(grep -c 'group-title="Live Event"' sirometv_updated.m3u || echo "0")
        echo "   üì∫ Live Event channels: $LIVE_EVENT_COUNT"
        
        # Move final file to main playlist
        mv sirometv_updated.m3u sirometv.m3u
        
        echo "‚úÖ Combined verification completed!"
        
    - name: Commit and push changes
      run: |
        echo "üíæ Saving combined changes to repository..."
        
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git add sirometv.m3u
        
        if git diff --staged --quiet; then
          echo "‚úÖ No changes detected - playlist is up to date"
        else
          # ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶®‡ßá‡¶ì‡ßü‡¶æ (BD Time format)
          BANGLADESH_TIME=$(TZ='Asia/Dhaka' date +'%Y-%m-%d %I:%M:%S%p BD Time')
          
          # Calculate channel counts
          ALL_CHANNEL_UPDATES=$(grep -c "#AUTO-UPDATED -.*‚úÖ-" sirometv.m3u || echo "0")
          ALL_CHANNEL_SKIPS=$(grep -c "#AUTO-UPDATED -.*‚õî SKIPPED ‚õî" sirometv.m3u || echo "0")
          ALL_CHANNEL_PARTIAL=$(grep -c "#AUTO-UPDATED -.*‚õî SKIPPED üü¢ Other Link üü¢" sirometv.m3u || echo "0")
          LIVE_EVENT_COUNT=$(grep -c 'group-title="Live Event"' sirometv.m3u || echo "0")
          
          COMMIT_MESSAGE="üîÑ Combined Update: "
          
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            COMMIT_MESSAGE+="Manual Run - "
          else
            COMMIT_MESSAGE+="Scheduled Run - "
          fi
          
          COMMIT_MESSAGE+="All Channel ($ALL_CHANNEL_UPDATES updates, $ALL_CHANNEL_PARTIAL partial, $ALL_CHANNEL_SKIPS skips) + Live Event ($LIVE_EVENT_COUNT channels) - $BANGLADESH_TIME"
          
          git commit -m "$COMMIT_MESSAGE"
          git push
          
          echo "üéâ Successfully updated combined playlist!"
          echo "üìä Summary:"
          echo "   All Channel üîÑ: $ALL_CHANNEL_UPDATES channels fully updated"
          echo "   All Channel üü°: $ALL_CHANNEL_PARTIAL channels partially updated"
          echo "   All Channel ‚õî: $ALL_CHANNEL_SKIPS channels skipped"
          echo "   Live Event: $LIVE_EVENT_COUNT channels"
          echo "   Time: $BANGLADESH_TIME"
        fi
        
    - name: Cleanup
      run: |
        echo "üßπ Cleaning up temporary files..."
        rm -f main_playlist.m3u main_playlist_backup.m3u
        rm -f source_playlist.m3u ayna_source.m3u fancode_source.m3u
        rm -f temp_after_all_channel.m3u
        echo "‚úÖ Cleanup completed!"
        
    - name: Success Notification
      if: success()
      run: |
        echo "üéä Combined workflow completed successfully!"
        echo "üìÖ Next update in 5 minutes"
        
    - name: Failure Notification
      if: failure()
      run: |
        echo "‚ùå Combined workflow failed!"
        echo "Please check the logs for errors."
        echo "Backup file may be available as main_playlist_backup.m3u"
