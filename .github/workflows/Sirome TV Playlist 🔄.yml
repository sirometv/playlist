name: Sirome TV Playlist üîÑ

on:
  schedule:
    - cron: '*/5 * * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update-playlists:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
        
    - name: Download all playlists
      run: |
        echo "üì• Downloading all required playlists..."
        
        # Source playlists
        curl -s -L -o source_playlist.m3u "https://raw.githubusercontent.com/sirometv/playlist/gh-pages/sirometv.m3u"
        curl -s -L -o ayna_source.m3u "https://raw.githubusercontent.com/sirometv/playlist/main/Ayna.m3u"
        curl -s -L -o fancode_source.m3u "https://raw.githubusercontent.com/abusaeeidx/Mrgify-BDIX-IPTV/main/playlist.m3u"
        
        # Verify downloads
        for file in source_playlist.m3u ayna_source.m3u fancode_source.m3u; do
          if [ ! -f "$file" ] || [ ! -s "$file" ]; then
            echo "‚ùå ERROR: $file download failed!"
            exit 1
          fi
          LINES=$(wc -l < "$file")
          echo "‚úÖ $file downloaded - $LINES lines"
        done
        
        # ‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨‡ßá ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶ï‡¶™‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
        cp source_playlist.m3u main_playlist.m3u
        echo "üìã Source playlist copied to main_playlist.m3u"
        
    - name: Backup original playlist
      run: |
        cp source_playlist.m3u main_playlist_backup.m3u
        echo "üì¶ Original playlist backed up"
        
    - name: Update All Channel üîÑ
      run: |
        echo "üîÑ Starting All Channel üîÑ update..."
        
        python3 - << 'EOF'
        import re
        from datetime import datetime
        import pytz
        import os
        
        def get_bangladesh_time():
            """‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡ßá"""
            bangladesh_tz = pytz.timezone('Asia/Dhaka')
            now = datetime.now(bangladesh_tz)
            return now.strftime("%Y-%m-%d %I:%M:%S%p BD Time")
        
        def extract_channels_from_source(source_file):
            """‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶§ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá"""
            channels = {}
            
            with open(source_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶¨‡ßç‡¶≤‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
            pattern = r'(#EXTINF:-1[^\n]*)\n((?:http[^\n]*\n)+)'
            matches = re.finditer(pattern, content)
            
            for match in matches:
                extinf_line = match.group(1)
                urls_block = match.group(2)
                
                # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ
                channel_name_match = re.search(r',([^,\n]+)$', extinf_line)
                if channel_name_match:
                    channel_name = channel_name_match.group(1).strip()
                    
                    # URL ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡¶æ
                    urls = [url.strip() for url in urls_block.strip().split('\n') if url.strip()]
                    
                    # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶§‡¶•‡ßç‡¶Ø
                    logo_match = re.search(r'tvg-logo="([^"]*)"', extinf_line)
                    group_match = re.search(r'group-title="([^"]*)"', extinf_line)
                    
                    channels[channel_name] = {
                        'extinf_line': extinf_line,
                        'urls': urls,
                        'tvg_logo': logo_match.group(1) if logo_match else "",
                        'group_title': group_match.group(1) if group_match else "",
                        'full_entry': match.group(0).strip()
                    }
            
            return channels
        
        def extract_channels_from_ayna(ayna_file):
            """Ayna ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶§ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá"""
            channels = {}
            
            with open(ayna_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ayna ‡¶•‡ßá‡¶ï‡ßá ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶¨‡ßç‡¶≤‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
            pattern = r'(#EXTINF:[^\n]*)\n((?:http[^\n]*\n)+)'
            matches = re.finditer(pattern, content)
            
            for match in matches:
                extinf_line = match.group(1)
                urls_block = match.group(2)
                
                # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ ‡¶è‡¶ï‡ßç‡¶∏‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ
                channel_name_match = re.search(r',([^,\n]+)$', extinf_line)
                if channel_name_match:
                    channel_name = channel_name_match.group(1).strip()
                    
                    # URL ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡¶æ
                    urls = [url.strip() for url in urls_block.strip().split('\n') if url.strip()]
                    
                    # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£
                    if channel_name not in channels:
                        channels[channel_name] = []
                    channels[channel_name].extend(urls)
                    channels[channel_name] = list(set(channels[channel_name]))  # ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠
            
            return channels
        
        def find_similar_channels(source_channel_name, ayna_channels):
            """‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶≤ ‡¶Ü‡¶õ‡ßá ‡¶è‡¶Æ‡¶® Ayna ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá"""
            similar_channels = []
            
            # ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö
            if source_channel_name in ayna_channels:
                similar_channels.append(source_channel_name)
            
            # ‡¶ï‡ßá‡¶∏-‡¶á‡¶®‡¶∏‡ßá‡¶®‡¶∏‡¶ø‡¶ü‡¶ø‡¶≠ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö
            lower_source = source_channel_name.lower()
            for ayna_channel in ayna_channels.keys():
                if ayna_channel.lower() == lower_source:
                    similar_channels.append(ayna_channel)
            
            # ‡¶™‡¶æ‡¶∞‡ßç‡¶∂‡¶ø‡ßü‡¶æ‡¶≤ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö (‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°)
            keywords = ['sports', 'tv', 'channel', 'bangla', 'bd']
            for ayna_channel in ayna_channels.keys():
                ayna_lower = ayna_channel.lower()
                source_lower = source_channel_name.lower()
                
                # ‡¶Ø‡¶¶‡¶ø ‡¶ï‡ßã‡¶® ‡¶ï‡ßÄ‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶° ‡¶õ‡¶æ‡ßú‡¶æ ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶Ø‡¶æ‡ßü
                common_words = set(source_lower.split()) & set(ayna_lower.split())
                if len(common_words) > 0 and ayna_channel not in similar_channels:
                    similar_channels.append(ayna_channel)
            
            return similar_channels
        
        def update_channel_in_main(main_content, channel_data, new_urls, ayna_channels):
            """‡¶Æ‡ßá‡¶á‡¶® ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßá"""
            timestamp = get_bangladesh_time()
            channel_name = next(iter(channel_data.keys()))
            data = channel_data[channel_name]
            
            # ‡¶è‡¶á ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø Ayna-‡¶§‡ßá ‡¶Æ‡¶ø‡¶≤ ‡¶Ü‡¶õ‡ßá ‡¶è‡¶Æ‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ
            similar_channels = find_similar_channels(channel_name, ayna_channels)
            
            # ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ URL ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ
            matched_urls = []
            unmatched_urls = []
            
            if similar_channels:
                # ‡¶∏‡¶¨ ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶•‡ßá‡¶ï‡ßá URL ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π
                all_ayna_urls = []
                for similar_channel in similar_channels:
                    all_ayna_urls.extend(ayna_channels[similar_channel])
                
                # ‡¶∏‡ßã‡¶∞‡ßç‡¶∏‡ßá‡¶∞ URL ‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶≤‡¶æ‡¶®‡ßã
                for url in data['urls']:
                    if url in all_ayna_urls:
                        matched_urls.append(url)
                    else:
                        unmatched_urls.append(url)
            else:
                # ‡¶ï‡ßã‡¶® ‡¶Æ‡¶ø‡¶≤ ‡¶®‡ßá‡¶á
                unmatched_urls = data['urls']
            
            # ‡¶®‡¶§‡ßÅ‡¶® URL ‡¶ó‡ßÅ‡¶≤‡ßã (‡¶Ø‡ßá‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∏‡ßã‡¶∞‡ßç‡¶∏‡ßá ‡¶®‡ßá‡¶á ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ Ayna-‡¶§‡ßá ‡¶Ü‡¶õ‡ßá)
            new_ayna_urls = []
            if similar_channels:
                for similar_channel in similar_channels:
                    for ayna_url in ayna_channels[similar_channel]:
                        if ayna_url not in data['urls']:
                            new_ayna_urls.append(ayna_url)
            
            # ‡¶∏‡¶¨ URL ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶§ ‡¶ï‡¶∞‡¶æ
            all_urls = []
            
            # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ URL ‡¶ó‡ßÅ‡¶≤‡ßã
            all_urls.extend(matched_urls)
            
            # ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶®‡¶§‡ßÅ‡¶® Ayna URL ‡¶ó‡ßÅ‡¶≤‡ßã
            all_urls.extend(new_ayna_urls)
            
            # ‡¶§‡¶æ‡¶∞‡¶™‡¶∞ ‡¶¨‡¶æ‡¶ï‡¶ø URL ‡¶ó‡ßÅ‡¶≤‡ßã
            all_urls.extend(unmatched_urls)
            
            # ‡¶á‡¶â‡¶®‡¶ø‡¶ï URL ‡¶∞‡¶æ‡¶ñ‡¶æ
            all_urls = list(dict.fromkeys(all_urls))
            
            # URL ‡¶¨‡ßç‡¶≤‡¶ï ‡¶§‡ßà‡¶∞‡¶ø
            url_block = '\n'.join(all_urls) + '\n'
            
            # ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶Æ‡ßá‡¶∏‡ßá‡¶ú ‡¶§‡ßà‡¶∞‡¶ø
            if matched_urls and new_ayna_urls:
                # ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶Æ‡¶ø‡¶≤‡ßá‡¶õ‡ßá, ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶®‡¶§‡ßÅ‡¶® ‡¶Ø‡ßã‡¶ó ‡¶π‡ßü‡ßá‡¶õ‡ßá
                update_message = f"#AUTO-UPDATED - {channel_name} ‚õî SKIPPED üü¢ Other Link üü¢ - {timestamp} üü° - üîÑ - üî¥\n"
            elif matched_urls and not new_ayna_urls:
                # ‡¶∏‡¶¨‡¶á ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶ó‡ßá‡¶õ‡ßá, ‡¶®‡¶§‡ßÅ‡¶® ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶®‡ßá‡¶á
                update_message = f"#AUTO-UPDATED - {channel_name} ‚õî SKIPPED ‚õî - {timestamp} üü° - üîÑ - üî¥\n"
            else:
                # ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á ‡¶Æ‡ßá‡¶≤‡ßá‡¶®‡¶ø, ‡¶∏‡¶¨ ‡¶®‡¶§‡ßÅ‡¶®
                update_message = f"#AUTO-UPDATED - {channel_name} ‚úÖ- {timestamp} üü° - üîÑ - üî¥\n"
            
            # ‡¶Æ‡ßá‡¶á‡¶® ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü‡ßá ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶∞‡¶ø‡¶™‡ßç‡¶≤‡ßá‡¶∏ ‡¶ï‡¶∞‡¶æ
            pattern = r'({})\n((?:http[^\n]*\n)+)'.format(re.escape(data['extinf_line']))
            
            def replacement(match):
                return match.group(1) + '\n' + update_message + url_block
            
            updated_content = re.sub(pattern, replacement, main_content, flags=re.MULTILINE)
            
            return updated_content, len(matched_urls), len(new_ayna_urls), len(unmatched_urls)
        
        # ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        source_channels = extract_channels_from_source('source_playlist.m3u')
        print(f"üìä Found {len(source_channels)} channels in source playlist")
        
        # Ayna ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        ayna_channels = extract_channels_from_ayna('ayna_source.m3u')
        print(f"üìä Found {len(ayna_channels)} channels in Ayna playlist")
        
        # ‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('main_playlist.m3u', 'r', encoding='utf-8') as f:
            main_content = f.read()
        
        # Check if manual run or scheduled
        is_manual_run = os.environ.get('GITHUB_EVENT_NAME') == 'workflow_dispatch'
        
        updated_content = main_content
        total_matched = 0
        total_new = 0
        total_skipped = 0
        
        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
        for channel_name, channel_data_dict in source_channels.items():
            print(f"\nüîç Processing: {channel_name}")
            
            # Ayna-‡¶§‡ßá ‡¶Æ‡¶ø‡¶≤ ‡¶Ü‡¶õ‡ßá ‡¶è‡¶Æ‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ
            similar_channels = find_similar_channels(channel_name, ayna_channels)
            
            if similar_channels:
                print(f"   Found similar channels in Ayna: {similar_channels}")
            else:
                print(f"   No similar channels found in Ayna")
            
            # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
            updated_content, matched_count, new_count, unmatched_count = update_channel_in_main(
                updated_content, 
                {channel_name: channel_data_dict}, 
                [], 
                ayna_channels
            )
            
            # ‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∏ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü
            total_matched += matched_count
            total_new += new_count
            
            if matched_count > 0 and new_count == 0:
                total_skipped += 1
                print(f"   ‚õî SKIPPED - {matched_count} URLs matched")
            elif matched_count > 0 and new_count > 0:
                print(f"   üü° PARTIAL - {matched_count} matched, {new_count} new URLs added")
            else:
                print(f"   ‚úÖ UPDATED - {new_count} new URLs added")
        
        # ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü temporary ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('temp_after_all_channel.m3u', 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        print(f"\nüéâ All Channel üîÑ Update Summary:")
        print(f"   Run type: {'Manual' if is_manual_run else 'Scheduled'}")
        print(f"   Total channels processed: {len(source_channels)}")
        print(f"   Total URLs matched: {total_matched}")
        print(f"   Total new URLs added: {total_new}")
        print(f"   Fully skipped channels: {total_skipped}")
        
        EOF
        
        echo "‚úÖ All Channel üîÑ update completed!"
        
    - name: Update Live Fancode üîÑ
      run: |
        echo "üîÑ Starting Live Fancode üîÑ update..."
        
        python3 - << 'EOF'
        import re
        from datetime import datetime
        import pytz
        
        # ‚öôÔ∏è CONFIGURATION
        TARGET_LINE_NUMBER = 25  # ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶á‡¶ö‡ßç‡¶õ‡¶æ‡¶Æ‡¶§ ‡¶≤‡¶æ‡¶á‡¶® ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®
        
        # ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤‡ßá‡¶∞ ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü
        TARGET_GROUP_TITLES = [
            "Live Event",
            # "Cricket",    # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Akash Go",    # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Football",    # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Sports",      # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Movies",      # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
        ]
        
        def get_bangladesh_time():
            """‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡ßá"""
            bangladesh_tz = pytz.timezone('Asia/Dhaka')
            now = datetime.now(bangladesh_tz)
            return now.strftime("%Y-%m-%d %I:%M:%S%p BD Time")
        
        def get_channel_name_strategy(group_title, extinf_line):
            """‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ‡ßá‡¶∞ strategy ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡ßá"""
            # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ extract ‡¶ï‡¶∞‡¶æ
            channel_name_match = re.search(r',([^,\n]+)$', extinf_line)
            original_name = channel_name_match.group(1).strip() if channel_name_match else ""
            
            if group_title == "Live Event":
                return "üü¢Live Matchüî¥"  # Live Event ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü ‡¶®‡¶æ‡¶Æ
            elif group_title == "Sports":
                return "Sports Live"  # Sports ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü ‡¶®‡¶æ‡¶Æ
            elif group_title == "Akash Go":
                return original_name  # Akash Go ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶∏‡¶≤ ‡¶®‡¶æ‡¶Æ ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá
            else:
                return original_name  # ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶∏‡¶≤ ‡¶®‡¶æ‡¶Æ
        
        def extract_group_channels(source_file, group_title):
            """‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ extract ‡¶ï‡¶∞‡ßá"""
            channels = []
            
            with open(source_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
            pattern = r'(#EXTINF:-1[^\n]*group-title="[^"]*{}[^"]*"[^\n]*\n)(http[^\n]+)'.format(re.escape(group_title))
            
            matches = re.finditer(pattern, content, re.IGNORECASE)
            
            for match in matches:
                extinf_line = match.group(1).strip()
                url_line = match.group(2).strip()
                
                # tvg-logo extract ‡¶ï‡¶∞‡¶æ
                logo_match = re.search(r'tvg-logo="([^"]*)"', extinf_line)
                tvg_logo = logo_match.group(1) if logo_match else "https://i.postimg.cc/d1hHy4ss/11.png"
                
                # ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£
                channel_name = get_channel_name_strategy(group_title, extinf_line)
                
                channels.append({
                    'extinf_line': extinf_line,
                    'url_line': url_line,
                    'tvg_logo': tvg_logo,
                    'channel_name': channel_name,
                    'group_title': group_title
                })
            
            print(f"‚úÖ Found {len(channels)} channels for group '{group_title}'")
            return channels
        
        def force_groups_at_line(main_content, all_group_channels, target_line):
            """‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶∏‡¶¨ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶∏‡ßá‡¶ï‡¶∂‡¶® ‡¶∞‡¶æ‡¶ñ‡ßá"""
            timestamp = get_bangladesh_time()
            
            # ‡¶∏‡¶¨ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø combined section ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
            new_combined_section = f"#AUTO-UPDATED - Multiple Groups - {timestamp}\n"
            
            total_channels = 0
            has_any_channels = False
            
            for group_title, channels in all_group_channels.items():
                if channels and len(channels) > 0:  # ‡¶∂‡ßÅ‡¶ß‡ßÅ channels ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶è‡¶¨‡¶Ç empty ‡¶®‡¶æ ‡¶π‡¶≤‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá
                    new_combined_section += f"# --- {group_title} Group ---\n"
                    
                    for channel in channels:
                        # ‡¶®‡¶§‡ßÅ‡¶® EXTINF ‡¶≤‡¶æ‡¶á‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ - ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ
                        new_extinf_line = f'#EXTINF:-1 tvg-logo="{channel["tvg_logo"]}" group-title="{group_title}", {channel["channel_name"]}\n'
                        new_combined_section += new_extinf_line
                        new_combined_section += channel["url_line"] + "\n"
                    
                    total_channels += len(channels)
                    has_any_channels = True
                    print(f"üì∫ Added {len(channels)} channels for {group_title}")
            
            # No channels found ‡¶≤‡¶æ‡¶á‡¶® ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶¨‡ßá ‡¶®‡¶æ
            if not has_any_channels:
                print("‚ÑπÔ∏è No channels found for any active group - creating empty AUTO-UPDATED section")
            
            # ‡¶Æ‡ßá‡¶á‡¶® ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü‡¶ï‡ßá ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ
            lines = main_content.split('\n')
            
            # TARGET_LINE_NUMBER validation
            if target_line < 1:
                target_line = 1
            if target_line > len(lines):
                target_line = len(lines)
            
            print(f"üìç Target line: {target_line} (Total lines: {len(lines)})")
            
            # existing ANY AUTO-UPDATED section ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶∞‡¶æ‡¶®‡ßã (‡¶™‡ßÅ‡¶∞‡¶æ‡¶®‡ßã ‡¶∏‡¶¨ versions)
            auto_updated_start = -1
            auto_updated_end = -1
            
            # existing section ‡¶è‡¶∞ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶è‡¶¨‡¶Ç ‡¶∂‡ßá‡¶∑ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ - ANY AUTO-UPDATED section
            for i, line in enumerate(lines):
                if '#AUTO-UPDATED' in line and 'Multiple Groups' in line:
                    auto_updated_start = i
                    print(f"üîç Found existing AUTO-UPDATED section at line {i+1}")
                    # section ‡¶è‡¶∞ ‡¶∂‡ßá‡¶∑ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ
                    for j in range(i + 1, len(lines)):
                        if not (lines[j].startswith('#EXTINF') or lines[j].startswith('http') or lines[j].startswith('# ---') or lines[j].strip() == ''):
                            auto_updated_end = j
                            break
                    if auto_updated_end == -1:
                        auto_updated_end = len(lines)
                    break
            
            if auto_updated_start != -1:
                print(f"üóëÔ∏è Removing existing AUTO-UPDATED section (lines {auto_updated_start+1}-{auto_updated_end})")
                # existing section ‡¶∏‡¶∞‡¶æ‡¶®‡ßã
                lines = lines[:auto_updated_start] + lines[auto_updated_end:]
            
            # ‡¶®‡¶§‡ßÅ‡¶® section ‡¶ü‡¶æ‡¶∞‡ßç‡¶ó‡ßá‡¶ü ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ
            insert_position = target_line - 1  # 0-based index
            
            # ‡¶Ø‡¶¶‡¶ø insert position existing removal ‡¶è‡¶∞ ‡¶™‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶π‡¶Ø‡¶º‡ßá ‡¶•‡¶æ‡¶ï‡ßá, adjust ‡¶ï‡¶∞‡¶æ
            if auto_updated_start != -1 and auto_updated_start < insert_position:
                insert_position -= (auto_updated_end - auto_updated_start)
            
            if insert_position > len(lines):
                insert_position = len(lines)
            
            new_lines = lines[:insert_position] + new_combined_section.split('\n') + lines[insert_position:]
            updated_content = '\n'.join(new_lines)
            
            print(f"‚úÖ Multiple Groups section positioned at line {target_line}")
            
            return updated_content, total_channels
        
        # All Channel update ‡¶è‡¶∞ ‡¶™‡¶∞‡ßá‡¶∞ temporary ‡¶´‡¶æ‡¶á‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('temp_after_all_channel.m3u', 'r', encoding='utf-8') as f:
            main_content = f.read()
        
        # Check if manual run or scheduled
        import os
        is_manual_run = os.environ.get('GITHUB_EVENT_NAME') == 'workflow_dispatch'
        
        # Filter out commented groups (lines starting with #)
        active_groups = [group for group in TARGET_GROUP_TITLES if not group.startswith('#')]
        
        if not active_groups:
            print("‚ùå ERROR: No active groups found! Please uncomment at least one group in TARGET_GROUP_TITLES")
            print("Current TARGET_GROUP_TITLES:", TARGET_GROUP_TITLES)
            exit(1)
        
        print(f"üéØ Active groups: {active_groups}")
        
        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø active ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ extract ‡¶ï‡¶∞‡ßÅ‡¶®
        all_group_channels = {}
        total_found_channels = 0
        has_any_channels_found = False
        
        for group_title in active_groups:
            print(f"\nüîç Processing group: {group_title}")
            channels = extract_group_channels('fancode_source.m3u', group_title)
            # ‡¶∂‡ßÅ‡¶ß‡ßÅ channels ‡¶•‡¶æ‡¶ï‡¶≤‡ßá all_group_channels ‡¶è ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá
            if channels:
                all_group_channels[group_title] = channels
                total_found_channels += len(channels)
                has_any_channels_found = True
            else:
                print(f"‚ÑπÔ∏è No channels found for group '{group_title}', skipping...")
        
        # ‡¶Ø‡¶¶‡¶ø ‡¶ï‡ßã‡¶® ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶á ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∂‡ßÅ‡¶ß‡ßÅ empty AUTO-UPDATED section ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá
        if not has_any_channels_found:
            print("‚ÑπÔ∏è No channels found for any active group, creating empty AUTO-UPDATED section...")
            # ‡¶∂‡ßÅ‡¶ß‡ßÅ timestamp ‡¶∏‡¶π AUTO-UPDATED ‡¶≤‡¶æ‡¶á‡¶® ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá
            all_group_channels = {}  # ‡¶ñ‡¶æ‡¶≤‡¶ø ‡¶∞‡¶æ‡¶ñ‡¶æ
        
        # ‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶® - ‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º TARGET_LINE_NUMBER ‡¶è ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá
        updated_content, total_added_channels = force_groups_at_line(main_content, all_group_channels, TARGET_LINE_NUMBER)
        
        # ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶´‡¶æ‡¶á‡¶®‡¶æ‡¶≤ ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('sirometv_updated.m3u', 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        print(f"\nüéâ Live Fancode üîÑ Update Summary:")
        print(f"   Run type: {'Manual' if is_manual_run else 'Scheduled'}")
        print(f"   Target line: {TARGET_LINE_NUMBER}")
        print(f"   Total active groups: {len(active_groups)}")
        print(f"   Total channels found: {total_found_channels}")
        print(f"   Total channels added: {total_added_channels}")
        print(f"   Configuration:")
        print(f"     - TARGET_LINE_NUMBER = {TARGET_LINE_NUMBER}")
        print(f"     - Active groups = {active_groups}")
        
        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ summary
        print(f"\nüìä Group-wise Summary:")
        for group_title in active_groups:
            if group_title in all_group_channels:
                channels = all_group_channels[group_title]
                channel_count = len(channels)
                status = "‚úÖ" if channel_count > 0 else "‚ùå"
                
                if channel_count > 0:
                    first_channel_name = channels[0]['channel_name']
                    naming_strategy = ""
                    if group_title == "Live Event":
                        naming_strategy = "(all named 'üü¢Live Matchüî¥')"
                    elif group_title == "Sports":
                        naming_strategy = "(all named 'Sports Live')"
                    elif group_title == "Akash Go":
                        naming_strategy = "(original names preserved)"
                    else:
                        naming_strategy = "(original names)"
                    
                    print(f"   {status} {group_title}: {channel_count} channels {naming_strategy}")
                    if group_title == "Akash Go" and channel_count > 0:
                        print(f"      Sample: {first_channel_name}")
                else:
                    print(f"   {status} {group_title}: {channel_count} channels")
            else:
                print(f"   ‚ùå {group_title}: 0 channels (not found in source)")
        
        EOF
        
        echo "‚úÖ Live Fancode üîÑ update completed!"
        
    - name: Verify combined updates
      run: |
        echo "üîç Verifying combined playlist updates..."
        
        if [ ! -f "sirometv_updated.m3u" ] || [ ! -s "sirometv_updated.m3u" ]; then
          echo "‚ùå ERROR: Updated playlist is missing or empty!"
          exit 1
        fi
        
        # Compare with backup
        OLD_LINES=$(wc -l < main_playlist_backup.m3u)
        NEW_LINES=$(wc -l < sirometv_updated.m3u)
        DIFF=$((NEW_LINES - OLD_LINES))
        
        echo "üìä Line count comparison:"
        echo "   Original: $OLD_LINES lines"
        echo "   Updated: $NEW_LINES lines"
        echo "   Difference: $DIFF lines"
        
        # Check All Channel updates
        echo "üîç Checking All Channel üîÑ updates:"
        ALL_CHANNEL_UPDATES=$(grep -c "#AUTO-UPDATED -.*‚úÖ-" sirometv_updated.m3u || echo "0")
        ALL_CHANNEL_SKIPS=$(grep -c "#AUTO-UPDATED -.*‚õî SKIPPED ‚õî" sirometv_updated.m3u || echo "0")
        ALL_CHANNEL_PARTIAL=$(grep -c "#AUTO-UPDATED -.*‚õî SKIPPED üü¢ Other Link üü¢" sirometv_updated.m3u || echo "0")
        echo "   ‚úÖ All Channel updates: $ALL_CHANNEL_UPDATES channels"
        echo "   ‚õî All Channel skips: $ALL_CHANNEL_SKIPS channels"
        echo "   üü° All Channel partial updates: $ALL_CHANNEL_PARTIAL channels"
        
        if [ "$ALL_CHANNEL_UPDATES" -eq "0" ] && [ "$ALL_CHANNEL_SKIPS" -eq "0" ] && [ "$ALL_CHANNEL_PARTIAL" -eq "0" ]; then
            echo "‚ö†Ô∏è Warning: No All Channel updates found"
        else
            echo "‚úÖ All Channel updates verified"
        fi
        
        # Check Live Fancode updates
        echo "üîç Checking Live Fancode üîÑ updates:"
        MULTIPLE_GROUPS_SECTION=$(grep -c "#AUTO-UPDATED - Multiple Groups" sirometv_updated.m3u || echo "0")
        echo "   Multiple Groups section: $MULTIPLE_GROUPS_SECTION"
        
        if [ "$MULTIPLE_GROUPS_SECTION" -eq "0" ]; then
            echo "‚ö†Ô∏è Warning: No Multiple Groups section found"
        else
            # Check section position
            SECTION_LINE=$(grep -n "#AUTO-UPDATED - Multiple Groups" sirometv_updated.m3u | cut -d: -f1)
            echo "   üìç Multiple Groups section at line: $SECTION_LINE"
            
            # Check if at correct line
            if [ "$SECTION_LINE" -ne "25" ]; then
                echo "‚ö†Ô∏è Warning: Multiple Groups section not at line 25 (found at line $SECTION_LINE)"
            else
                echo "‚úÖ Multiple Groups section correctly positioned at line 25"
            fi
        fi
        
        # Check for Live Event group
        LIVE_EVENT_COUNT=$(grep -c 'group-title="Live Event"' sirometv_updated.m3u || echo "0")
        echo "   üì∫ Live Event channels: $LIVE_EVENT_COUNT"
        
        # Verify no "No channels found" lines
        if grep -q "No channels found" sirometv_updated.m3u; then
            echo "‚ùå ERROR: 'No channels found' line still exists!"
            exit 1
        else
            echo "‚úÖ 'No channels found' line is completely removed!"
        fi
        
        # Check for multiple URLs in channels
        echo "üîç Checking for multiple URLs in channels:"
        MULTI_URL_CHANNELS=$(grep -A 5 "#AUTO-UPDATED" sirometv_updated.m3u | grep -c "^http" || echo "0")
        echo "   Total URL lines in AUTO-UPDATED sections: $MULTI_URL_CHANNELS"
        
        # Move final file to main playlist
        mv sirometv_updated.m3u sirometv.m3u
        
        echo "‚úÖ Combined verification completed!"
        
    - name: Commit and push changes
      run: |
        echo "üíæ Saving combined changes to repository..."
        
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git add sirometv.m3u
        
        if git diff --staged --quiet; then
          echo "‚úÖ No changes detected - playlist is up to date"
        else
          # ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶®‡ßá‡¶ì‡ßü‡¶æ (BD Time format)
          BANGLADESH_TIME=$(TZ='Asia/Dhaka' date +'%Y-%m-%d %I:%M:%S%p BD Time')
          
          # Calculate channel counts
          ALL_CHANNEL_UPDATES=$(grep -c "#AUTO-UPDATED -.*‚úÖ-" sirometv.m3u || echo "0")
          ALL_CHANNEL_SKIPS=$(grep -c "#AUTO-UPDATED -.*‚õî SKIPPED ‚õî" sirometv.m3u || echo "0")
          ALL_CHANNEL_PARTIAL=$(grep -c "#AUTO-UPDATED -.*‚õî SKIPPED üü¢ Other Link üü¢" sirometv.m3u || echo "0")
          LIVE_EVENT_COUNT=$(grep -c 'group-title="Live Event"' sirometv.m3u || echo "0")
          
          COMMIT_MESSAGE="üîÑ Combined Update: "
          
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            COMMIT_MESSAGE+="Manual Run - "
          else
            COMMIT_MESSAGE+="Scheduled Run - "
          fi
          
          COMMIT_MESSAGE+="All Channel ($ALL_CHANNEL_UPDATES updates, $ALL_CHANNEL_PARTIAL partial, $ALL_CHANNEL_SKIPS skips) + Live Event ($LIVE_EVENT_COUNT channels) - $BANGLADESH_TIME"
          
          git commit -m "$COMMIT_MESSAGE"
          git push
          
          echo "üéâ Successfully updated combined playlist!"
          echo "üìä Summary:"
          echo "   All Channel üîÑ: $ALL_CHANNEL_UPDATES channels fully updated"
          echo "   All Channel üü°: $ALL_CHANNEL_PARTIAL channels partially updated"
          echo "   All Channel ‚õî: $ALL_CHANNEL_SKIPS channels skipped"
          echo "   Live Event: $LIVE_EVENT_COUNT channels"
          echo "   Time: $BANGLADESH_TIME"
        fi
        
    - name: Cleanup
      run: |
        echo "üßπ Cleaning up temporary files..."
        rm -f main_playlist.m3u main_playlist_backup.m3u
        rm -f source_playlist.m3u ayna_source.m3u fancode_source.m3u
        rm -f temp_after_all_channel.m3u
        echo "‚úÖ Cleanup completed!"
        
    - name: Success Notification
      if: success()
      run: |
        echo "üéä Combined workflow completed successfully!"
        echo "üìÖ Next update in 5 minutes"
        
    - name: Failure Notification
      if: failure()
      run: |
        echo "‚ùå Combined workflow failed!"
        echo "Please check the logs for errors."
        echo "Backup file may be available as main_playlist_backup.m3u"
