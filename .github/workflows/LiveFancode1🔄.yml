name: Live Fancode üîÑ

on:
  schedule:
    - cron: '*/5 * * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  sync-channels:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Download playlists
      run: |
        echo "üì• Downloading playlists..."
        curl -s -L -o main_playlist.m3u "https://raw.githubusercontent.com/sirometv/playlist/main/sirometvsports.m3u"
        curl -s -L -o source_playlist.m3u "https://raw.githubusercontent.com/abusaeeidx/Mrgify-BDIX-IPTV/main/playlist.m3u"
        
        for file in main_playlist.m3u source_playlist.m3u; do
          if [ ! -f "$file" ] || [ ! -s "$file" ]; then
            echo "‚ùå ERROR: $file download failed!"
            exit 1
          fi
          LINES=$(wc -l < "$file")
          echo "‚úÖ $file downloaded - $LINES lines"
        done
        
    - name: Update Fancode Groups
      run: |
        echo "üîÑ Updating Fancode Channels..."
        
        python3 - << 'EOF'
        import re
        from datetime import datetime
        import pytz
        import urllib.parse
        
        # ‚öôÔ∏è CONFIGURATION - ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶≤‡¶æ‡¶á‡¶® ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤‡¶ó‡ßÅ‡¶≤‡ßã ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá‡¶®
        TARGET_LINE_NUMBER = 37  # ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶á‡¶ö‡ßç‡¶õ‡¶æ‡¶Æ‡¶§ ‡¶≤‡¶æ‡¶á‡¶® ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®
        
        # ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤‡ßá‡¶∞ ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü - ‡¶Ø‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ñ‡ßÅ‡¶∂‡¶ø ‡¶Ø‡ßã‡¶ó/‡¶¨‡¶ø‡ßü‡ßã‡¶ó ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá‡¶®
        # ‡¶ï‡¶Æ‡¶™‡¶ï‡ßç‡¶∑‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ active ‡¶•‡¶æ‡¶ï‡¶§‡ßá ‡¶π‡¶¨‡ßá (uncomment ‡¶ï‡¶∞‡¶æ)
        TARGET_GROUP_TITLES = [
            "Live Event",
            # "Cricket",    # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Akash Go",    # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Football",    # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Sports",      # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
            # "Movies",      # uncomment ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶® ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü
        ]
        
        def get_bangladesh_time():
            """‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡ßá"""
            bangladesh_tz = pytz.timezone('Asia/Dhaka')
            now = datetime.now(bangladesh_tz)
            return now.strftime("%Y-%m-%d %I:%M:%S%p BD Time")
        
        def resize_tv_logo(logo_url, width=300, height=180):
            """
            ‡¶ü‡¶ø‡¶≠‡¶ø ‡¶≤‡ßã‡¶ó‡ßã URL ‡¶∞‡¶ø‡¶∏‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡ßá‡•§ 
            ‡¶Ø‡¶¶‡¶ø ‡¶á‡¶§‡¶ø‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá resized URL ‡¶•‡¶æ‡¶ï‡ßá ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∏‡ßá‡¶ü‡¶æ‡¶á ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡ßá,
            ‡¶®‡¶æ‡¶π‡¶≤‡ßá resize parameters ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßá‡•§
            
            Args:
                logo_url: ‡¶Æ‡ßÇ‡¶≤ ‡¶≤‡ßã‡¶ó‡ßã URL
                width: ‡¶ö‡ßÇ‡ßú‡¶æ‡¶®‡ßç‡¶§ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶• (‡¶™‡¶ø‡¶ï‡ßç‡¶∏‡ßá‡¶≤‡ßá)
                height: ‡¶ö‡ßÇ‡ßú‡¶æ‡¶®‡ßç‡¶§ ‡¶â‡¶ö‡ßç‡¶ö‡¶§‡¶æ (‡¶™‡¶ø‡¶ï‡ßç‡¶∏‡ßá‡¶≤‡ßá)
            
            Returns:
                resized ‡¶≤‡ßã‡¶ó‡ßã URL
            """
            if not logo_url or logo_url.strip() == "":
                return "https://i.postimg.cc/d1hHy4ss/11.png"
            
            # URL ‡¶°‡¶ø‡¶ï‡ßã‡¶° ‡¶ï‡¶∞‡¶æ
            decoded_url = logo_url
            
            # ‡¶á‡¶§‡¶ø‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá resize ‡¶ï‡¶∞‡¶æ ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ
            resize_patterns = [
                r'=s\d+',  # Google Photos pattern
                r'/width/\d+',  # Common resize patterns
                r'w=\d+',  # width parameter
                r'h=\d+',  # height parameter
                r'size/\d+',  # size parameter
                r'resize/\d+',  # resize parameter
            ]
            
            for pattern in resize_patterns:
                if re.search(pattern, decoded_url, re.IGNORECASE):
                    # ‡¶á‡¶§‡¶ø‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá resize ‡¶ï‡¶∞‡¶æ ‡¶Ü‡¶õ‡ßá
                    return decoded_url
            
            # ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® CDN/Image hosting services ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø resize ‡¶≤‡¶ú‡¶ø‡¶ï
            if 'postimg.cc' in decoded_url:
                # PostImage ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø resize
                if '/gallery/' in decoded_url:
                    # Direct image link ‡¶π‡¶≤‡ßá
                    return decoded_url.replace('/gallery/', f'/gallery/{width}x{height}/')
                else:
                    return decoded_url
            
            elif 'imgur.com' in decoded_url:
                # Imgur ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø resize
                if '.jpg' in decoded_url or '.png' in decoded_url or '.jpeg' in decoded_url:
                    # Direct image link ‡¶π‡¶≤‡ßá
                    base_url = decoded_url.rsplit('.', 1)[0]
                    extension = decoded_url.rsplit('.', 1)[1]
                    return f"{base_url}b.{extension}"  # Imgur ‡¶è‡¶∞ 'b' suffix medium size
            
            elif 'googleusercontent.com' in decoded_url:
                # Google Photos/Drive ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø resize
                if '=s' in decoded_url:
                    # ‡¶á‡¶§‡¶ø‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá size parameter ‡¶Ü‡¶õ‡ßá
                    return re.sub(r'=s\d+', f'=s{max(width, height)}', decoded_url)
                else:
                    # size parameter ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ
                    return f"{decoded_url}=s{max(width, height)}"
            
            elif 'cloudinary.com' in decoded_url:
                # Cloudinary ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø resize
                if '/upload/' in decoded_url:
                    parts = decoded_url.split('/upload/')
                    if len(parts) == 2:
                        return f"{parts[0]}/upload/w_{width},h_{height},c_fill/{parts[1]}"
            
            elif 'imageshack.com' in decoded_url or 'tinypic.com' in decoded_url:
                # ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø image hosting services
                return decoded_url
            
            elif 'githubusercontent.com' in decoded_url:
                # GitHub raw content
                return decoded_url
            
            # Default: ‡¶Ø‡¶¶‡¶ø ‡¶ï‡ßã‡¶®‡ßã specific service ‡¶®‡¶æ ‡¶Æ‡ßá‡¶≤‡ßá, ‡¶§‡¶¨‡ßÅ‡¶ì original URL ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡¶¨‡ßá
            # ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶Ö‡¶®‡ßá‡¶ï IPTV ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø resize support ‡¶ï‡¶∞‡ßá ‡¶®‡¶æ
            return decoded_url
        
        def get_channel_name_strategy(group_title, extinf_line):
            """‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ‡ßá‡¶∞ strategy ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡ßá"""
            # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ extract ‡¶ï‡¶∞‡¶æ
            channel_name_match = re.search(r',([^,\n]+)$', extinf_line)
            original_name = channel_name_match.group(1).strip() if channel_name_match else ""
            
            if group_title == "Live Event":
                return "Fancode"  # Live Event ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü ‡¶®‡¶æ‡¶Æ
            elif group_title == "Sports":
                return "Sports Live"  # Sports ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶°‡¶ø‡¶´‡¶≤‡ßç‡¶ü ‡¶®‡¶æ‡¶Æ
            elif group_title == "Akash Go":
                return original_name  # Akash Go ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶∏‡¶≤ ‡¶®‡¶æ‡¶Æ ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá
            else:
                return original_name  # ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶∏‡¶≤ ‡¶®‡¶æ‡¶Æ
        
        def extract_group_channels(source_file, group_title):
            """‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ extract ‡¶ï‡¶∞‡ßá"""
            channels = []
            
            with open(source_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
            pattern = r'(#EXTINF:-1[^\n]*group-title="[^"]*{}[^"]*"[^\n]*\n)(http[^\n]+)'.format(re.escape(group_title))
            
            matches = re.finditer(pattern, content, re.IGNORECASE)
            
            for match in matches:
                extinf_line = match.group(1).strip()
                url_line = match.group(2).strip()
                
                # tvg-logo extract ‡¶ï‡¶∞‡¶æ
                logo_match = re.search(r'tvg-logo="([^"]*)"', extinf_line)
                original_logo = logo_match.group(1) if logo_match else ""
                
                # ‡¶≤‡ßã‡¶ó‡ßã ‡¶∞‡¶ø‡¶∏‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡¶æ (300x180px)
                resized_logo = resize_tv_logo(original_logo, width=300, height=180)
                
                # ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£
                channel_name = get_channel_name_strategy(group_title, extinf_line)
                
                channels.append({
                    'extinf_line': extinf_line,
                    'url_line': url_line,
                    'tvg_logo': resized_logo,
                    'channel_name': channel_name,
                    'group_title': group_title,
                    'original_logo': original_logo  # ‡¶°‡¶ø‡¶¨‡¶æ‡¶ó‡¶ø‡¶Ç ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶£
                })
            
            print(f"‚úÖ Found {len(channels)} channels for group '{group_title}'")
            return channels
        
        def force_groups_at_line(main_content, all_group_channels, target_line):
            """‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶∏‡¶¨ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶∏‡ßá‡¶ï‡¶∂‡¶® ‡¶∞‡¶æ‡¶ñ‡ßá"""
            timestamp = get_bangladesh_time()
            
            # ‡¶∏‡¶¨ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø combined section ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ
            new_combined_section = f"#AUTO-UPDATED - Multiple Groups - {timestamp}\n"
            
            total_channels = 0
            has_any_channels = False
            
            for group_title, channels in all_group_channels.items():
                if channels and len(channels) > 0:  # ‡¶∂‡ßÅ‡¶ß‡ßÅ channels ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶è‡¶¨‡¶Ç empty ‡¶®‡¶æ ‡¶π‡¶≤‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá
                    new_combined_section += f"# --- {group_title} Group ---\n"
                    
                    for channel in channels:
                        # ‡¶®‡¶§‡ßÅ‡¶® EXTINF ‡¶≤‡¶æ‡¶á‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ - ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ
                        new_extinf_line = f'#EXTINF:-1 tvg-logo="{channel["tvg_logo"]}" group-title="{group_title}", {channel["channel_name"]}\n'
                        new_combined_section += new_extinf_line
                        new_combined_section += channel["url_line"] + "\n"
                    
                    total_channels += len(channels)
                    has_any_channels = True
                    print(f"üì∫ Added {len(channels)} channels for {group_title}")
            
            # ‡¶Ø‡¶¶‡¶ø ‡¶ï‡ßã‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá
            if not has_any_channels:
                print("‚ÑπÔ∏è No channels found for any active group - creating empty AUTO-UPDATED section")
            
            # ‡¶Æ‡ßá‡¶á‡¶® ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü‡¶ï‡ßá ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ
            lines = main_content.split('\n')
            
            # TARGET_LINE_NUMBER validation
            if target_line < 1:
                target_line = 1
            if target_line > len(lines):
                target_line = len(lines)
            
            print(f"üìç Target line: {target_line} (Total lines: {len(lines)})")
            
            # existing ANY AUTO-UPDATED section ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶∞‡¶æ‡¶®‡ßã (‡¶™‡ßÅ‡¶∞‡¶æ‡¶®‡ßã ‡¶∏‡¶¨ versions)
            auto_updated_start = -1
            auto_updated_end = -1
            
            # existing section ‡¶è‡¶∞ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶è‡¶¨‡¶Ç ‡¶∂‡ßá‡¶∑ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ - ANY AUTO-UPDATED section
            for i, line in enumerate(lines):
                if '#AUTO-UPDATED' in line:
                    auto_updated_start = i
                    print(f"üîç Found existing AUTO-UPDATED section at line {i+1}")
                    # section ‡¶è‡¶∞ ‡¶∂‡ßá‡¶∑ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶æ
                    for j in range(i + 1, len(lines)):
                        if not (lines[j].startswith('#EXTINF') or lines[j].startswith('http') or lines[j].startswith('# ---') or lines[j].strip() == ''):
                            auto_updated_end = j
                            break
                    if auto_updated_end == -1:
                        auto_updated_end = len(lines)
                    break
            
            if auto_updated_start != -1:
                print(f"üóëÔ∏è Removing existing AUTO-UPDATED section (lines {auto_updated_start+1}-{auto_updated_end})")
                # existing section ‡¶∏‡¶∞‡¶æ‡¶®‡ßã
                lines = lines[:auto_updated_start] + lines[auto_updated_end:]
            
            # ‡¶®‡¶§‡ßÅ‡¶® section ‡¶ü‡¶æ‡¶∞‡ßç‡¶ó‡ßá‡¶ü ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ
            insert_position = target_line - 1  # 0-based index
            
            # ‡¶Ø‡¶¶‡¶ø insert position existing removal ‡¶è‡¶∞ ‡¶™‡¶∞ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶π‡¶Ø‡¶º‡ßá ‡¶•‡¶æ‡¶ï‡ßá, adjust ‡¶ï‡¶∞‡¶æ
            if auto_updated_start != -1 and auto_updated_start < insert_position:
                insert_position -= (auto_updated_end - auto_updated_start)
            
            if insert_position > len(lines):
                insert_position = len(lines)
            
            new_lines = lines[:insert_position] + new_combined_section.split('\n') + lines[insert_position:]
            updated_content = '\n'.join(new_lines)
            
            print(f"‚úÖ Multiple Groups section positioned at line {target_line}")
            
            return updated_content, total_channels
        
        # ‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('main_playlist.m3u', 'r', encoding='utf-8') as f:
            main_content = f.read()
        
        # Check if manual run or scheduled
        import os
        is_manual_run = os.environ.get('GITHUB_EVENT_NAME') == 'workflow_dispatch'
        
        # Filter out commented groups (lines starting with #)
        active_groups = [group for group in TARGET_GROUP_TITLES if not group.startswith('#')]
        
        if not active_groups:
            print("‚ùå ERROR: No active groups found! Please uncomment at least one group in TARGET_GROUP_TITLES")
            print("Current TARGET_GROUP_TITLES:", TARGET_GROUP_TITLES)
            exit(1)
        
        print(f"üéØ Active groups: {active_groups}")
        
        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø active ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶ü‡¶æ‡¶á‡¶ü‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ extract ‡¶ï‡¶∞‡ßÅ‡¶®
        all_group_channels = {}
        total_found_channels = 0
        has_any_channels_found = False
        
        for group_title in active_groups:
            print(f"\nüîç Processing group: {group_title}")
            channels = extract_group_channels('source_playlist.m3u', group_title)
            # ‡¶∂‡ßÅ‡¶ß‡ßÅ channels ‡¶•‡¶æ‡¶ï‡¶≤‡ßá all_group_channels ‡¶è ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá
            if channels:
                all_group_channels[group_title] = channels
                total_found_channels += len(channels)
                has_any_channels_found = True
            else:
                print(f"‚ÑπÔ∏è No channels found for group '{group_title}', skipping...")
        
        # ‡¶Ø‡¶¶‡¶ø ‡¶ï‡ßã‡¶® ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶á ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∂‡ßÅ‡¶ß‡ßÅ empty AUTO-UPDATED section ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá
        if not has_any_channels_found:
            print("‚ÑπÔ∏è No channels found for any active group, creating empty AUTO-UPDATED section...")
            # ‡¶∂‡ßÅ‡¶ß‡ßÅ timestamp ‡¶∏‡¶π AUTO-UPDATED ‡¶≤‡¶æ‡¶á‡¶® ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá
            all_group_channels = {}  # ‡¶ñ‡¶æ‡¶≤‡¶ø ‡¶∞‡¶æ‡¶ñ‡¶æ
        
        # ‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶® - ‡¶∏‡¶¨‡¶∏‡¶Æ‡¶Ø‡¶º TARGET_LINE_NUMBER ‡¶è ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá
        updated_content, total_added_channels = force_groups_at_line(main_content, all_group_channels, TARGET_LINE_NUMBER)
        
        # ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('sirometv_updated.m3u', 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        print(f"\nüéâ Update Summary:")
        print(f"   Run type: {'Manual' if is_manual_run else 'Scheduled'}")
        print(f"   Target line: {TARGET_LINE_NUMBER}")
        print(f"   Total active groups: {len(active_groups)}")
        print(f"   Total channels found: {total_found_channels}")
        print(f"   Total channels added: {total_added_channels}")
        print(f"   Configuration:")
        print(f"     - TARGET_LINE_NUMBER = {TARGET_LINE_NUMBER}")
        print(f"     - Active groups = {active_groups}")
        
        # ‡¶≤‡ßã‡¶ó‡ßã ‡¶∞‡¶ø‡¶∏‡¶æ‡¶á‡¶ú‡¶ø‡¶Ç ‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∏
        print(f"\nüñºÔ∏è Logo Resizing Status:")
        print(f"   All logos resized to: 300px √ó 180px")
        
        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ summary
        print(f"\nüìä Group-wise Summary:")
        for group_title in active_groups:
            if group_title in all_group_channels:
                channels = all_group_channels[group_title]
                channel_count = len(channels)
                status = "‚úÖ" if channel_count > 0 else "‚ùå"
                
                if channel_count > 0:
                    first_channel_name = channels[0]['channel_name']
                    first_channel_logo = channels[0]['tvg_logo']
                    naming_strategy = ""
                    if group_title == "Live Event":
                        naming_strategy = "(all named 'Fancode')"
                    elif group_title == "Sports":
                        naming_strategy = "(all named 'Sports Live')"
                    elif group_title == "Akash Go":
                        naming_strategy = "(original names preserved)"
                    else:
                        naming_strategy = "(original names)"
                    
                    print(f"   {status} {group_title}: {channel_count} channels {naming_strategy}")
                    print(f"      Sample logo URL: {first_channel_logo[:80]}...")
                else:
                    print(f"   {status} {group_title}: {channel_count} channels")
            else:
                print(f"   ‚ùå {group_title}: 0 channels (not found in source)")
        
        EOF
        
    - name: Verify and replace playlist
      run: |
        echo "üîç Verifying updated playlist..."
        
        if [ ! -f "sirometv_updated.m3u" ] || [ ! -s "sirometv_updated.m3u" ]; then
          echo "‚ùå ERROR: Updated playlist is missing or empty!"
          exit 1
        fi
        
        # ‡¶≤‡¶æ‡¶á‡¶® ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ
        OLD_LINES=$(wc -l < main_playlist.m3u)
        NEW_LINES=$(wc -l < sirometv_updated.m3u)
        echo "üìä Line count - Old: $OLD_LINES, New: $NEW_LINES, Difference: $((NEW_LINES - OLD_LINES))"
        
        # AUTO-UPDATED section ‡¶è‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ
        echo "üìç Checking AUTO-UPDATED sections:"
        grep -n "AUTO-UPDATED" sirometv_updated.m3u
        
        # No channels found ‡¶≤‡¶æ‡¶á‡¶® ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ
        echo "üîç Checking for 'No channels found' line:"
        if grep -q "No channels found" sirometv_updated.m3u; then
          echo "‚ùå ERROR: 'No channels found' line still exists!"
          exit 1
        else
          echo "‚úÖ 'No channels found' line is completely removed!"
        fi
        
        # ‡¶≤‡ßã‡¶ó‡ßã URL ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ
        echo "üîç Checking logo URLs in EXTINF lines:"
        LOGO_COUNT=$(grep -c 'tvg-logo="' sirometv_updated.m3u || echo "0")
        echo "‚úÖ Total logo URLs found: $LOGO_COUNT"
        
        # ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶∞‡¶ø‡¶∏‡¶æ‡¶á‡¶ú‡¶° ‡¶≤‡ßã‡¶ó‡ßã URL ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã
        echo "üì∏ Sample resized logo URLs:"
        grep -m 3 'tvg-logo="' sirometv_updated.m3u | head -3 | while read line; do
          LOGO_URL=$(echo "$line" | grep -o 'tvg-logo="[^"]*"' | cut -d'"' -f2)
          if [ ! -z "$LOGO_URL" ]; then
            echo "   $LOGO_URL"
          fi
        done
        
        mv sirometv_updated.m3u sirometvsports.m3u
        
        # ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®
        echo "üîç Updated groups and channels:"
        for group in "Live Event" "Akash Go" "Sports" "Cricket" "Football" "Movies"; do
            COUNT=$(grep -c "group-title=\"$group\"" sirometvsports.m3u || echo "0")
            if [ "$COUNT" -gt "0" ]; then
                echo "üì∫ $group group: $COUNT channels"
                # ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶≤‡ßã‡¶ó‡ßã URL ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã
                grep -A 1 "group-title=\"$group\"" sirometvsports.m3u | head -2 | while read line; do
                  if echo "$line" | grep -q 'tvg-logo="'; then
                    LOGO_URL=$(echo "$line" | grep -o 'tvg-logo="[^"]*"' | cut -d'"' -f2)
                    echo "   Logo: $LOGO_URL"
                  fi
                done
            fi
        done
        
        # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶®‡¶æ‡¶Æ verification
        echo "üîç Channel name verification:"
        if grep -q "group-title=\"Live Event\"" sirometvsports.m3u; then
            echo "‚úÖ Live Event channels: All named 'Fancode'"
        else
            echo "‚ÑπÔ∏è No Live Event channels found"
        fi
        
        echo "‚úÖ Multiple groups section successfully updated with resized logos!"
        
    - name: Commit and push changes
      run: |
        echo "üíæ Saving changes to repository..."
        
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git add sirometvsports.m3u
        
        if git diff --staged --quiet; then
          echo "‚úÖ No changes detected - playlist is up to date"
        else
          BANGLADESH_TIME=$(TZ='Asia/Dhaka' date +'%Y-%m-%d %I:%M:%S%p BD Time')
          
          # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá‡¶∞ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ó‡¶£‡¶®‡¶æ
          LIVE_EVENT_COUNT=$(grep -c 'group-title="Live Event"' sirometvsports.m3u || echo "0")
          
          COMMIT_MESSAGE="üîÑ "
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            COMMIT_MESSAGE+="Manual Update"
          else
            COMMIT_MESSAGE+="Scheduled Update"
          fi
          
          COMMIT_MESSAGE+=": Live Event Group"
          if [ "$LIVE_EVENT_COUNT" -gt "0" ]; then
            COMMIT_MESSAGE+=" ($LIVE_EVENT_COUNT channels)"
          else
            COMMIT_MESSAGE+=" (No channels found)"
          fi
          
          COMMIT_MESSAGE+=" with 300x180 Logos"
          COMMIT_MESSAGE+=" - $BANGLADESH_TIME"
          
          git commit -m "$COMMIT_MESSAGE"
          git push
          
          echo "üéâ Successfully updated playlist at line 37!"
          if [ "$LIVE_EVENT_COUNT" -gt "0" ]; then
            echo "   Live Event: $LIVE_EVENT_COUNT channels (named 'Fancode')"
            echo "   Logos resized to: 300px √ó 180px"
          else
            echo "   ‚ÑπÔ∏è No Live Event channels found in source"
          fi
        fi
        
    - name: Cleanup
      run: |
        rm -f main_playlist.m3u source_playlist.m3u
        echo "‚úÖ Cleanup completed!"
