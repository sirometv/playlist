name: Ayna To Other

on:
  schedule:
    - cron: '*/5 * * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  sync-channels:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Download playlists
      run: |
        echo "üì• Downloading playlists..."
        curl -s -L -o main_playlist.m3u "https://raw.githubusercontent.com/sirometv/playlist/main/sirometv.m3u"
        curl -s -L -o source_playlist.m3u "https://raw.githubusercontent.com/sirometv/playlist/main/Ayna.m3u"
        
        for file in main_playlist.m3u source_playlist.m3u; do
          if [ ! -f "$file" ] || [ ! -s "$file" ]; then
            echo "‚ùå ERROR: $file download failed!"
            exit 1
          fi
          LINES=$(wc -l < "$file")
          echo "‚úÖ $file downloaded - $LINES lines"
        done
        
    - name: Update Channels
      run: |
        echo "üîÑ Updating Channels..."
        
        python3 - << 'EOF'
        import re
        from datetime import datetime
        import pytz
        import os
        
        # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶™‡¶ø‡¶Ç - ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ : ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶° ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü‡ßá ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶¨‡ßá ‡¶è‡¶Æ‡¶® exact keywords
        CHANNEL_MAPPING = {
            "T Sports": ["T Sports HD", "A Sports", "PTV Sports"],
            "BTV National": ["BTV NATIONAL HD", "BTV World"],
            "BTV CTG": ["BTV CTG"],
            "Channel 9": ["Channel 9"],
            "Channel I": ["Channel I"],
            "Bangla Vision": ["Bangla Vision"],
            "News 24": ["News 24 BD"], 
            "Boishakhi TV": ["Boishakhi TV"],
            "Bijoy TV": ["Bijoy TV"],
            "ATN Bangla": ["ATN Bangla"],
            "ATN-Bangla News": ["ATN News"],
            "DBC News": ["DBC News"],
            "Independent TV": ["Independent TV"],
            "Bangla TV": ["Bangla TV"],
            "Desh TV": ["Desh TV"],
            "RTV": ["RTV"],
            "My TV": ["My TV"],
        }
        
        def get_bangladesh_time():
            """‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶ï‡¶∞‡ßá"""
            bangladesh_tz = pytz.timezone('Asia/Dhaka')
            now = datetime.now(bangladesh_tz)
            return now.strftime("%Y-%m-%d %I:%M:%S%p BD Time")
        
        def find_channel_urls(source_file, keywords):
            """‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ URLs ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá (exact match)"""
            with open(source_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            urls = []
            
            # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø keyword ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø exact match ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßÅ‡¶®
            for keyword in keywords:
                # Exact match ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø pattern - ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶®‡¶æ‡¶Æ match ‡¶ï‡¶∞‡¶¨‡ßá
                pattern = r'#EXTINF:[^\n]*,\s*{}\s*(?:\n|$)[^\n]*\n(http[^\s\n]+)'.format(re.escape(keyword))
                matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
                
                if matches:
                    for url in matches:
                        urls.append(url.strip())
                        print(f"‚úÖ Found exact match for '{keyword}': {url}")
                else:
                    print(f"‚ùå No exact match found for keyword: '{keyword}'")
            
            return urls
        
        def extract_existing_urls(channel_content):
            """‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá existing URLs extract ‡¶ï‡¶∞‡ßá"""
            # ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ URL ‡¶≤‡¶æ‡¶á‡¶®‡¶ó‡ßÅ‡¶≤‡ßã extract ‡¶ï‡¶∞‡ßÅ‡¶® (‡¶ï‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶õ‡¶æ‡ßú‡¶æ)
            url_pattern = r'^(https?://[^\s#]+)$'
            urls = re.findall(url_pattern, channel_content, re.MULTILINE)
            return set(urls)
        
        def update_channel_in_playlist(main_content, channel_name, new_urls):
            """‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßá"""
            timestamp = get_bangladesh_time()
            is_manual_run = os.environ.get('GITHUB_EVENT_NAME') == 'workflow_dispatch'
            
            print(f"üîÑ Processing channel: {channel_name}")
            print(f"   New URLs found: {len(new_urls)}")
            print(f"   Run type: {'Manual' if is_manual_run else 'Scheduled'}")
            
            # ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶¨‡ßç‡¶≤‡¶ï ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ pattern
            channel_pattern = r'(#EXTINF:-1[^\n]*{}[^\n]*\n)(.*?)(?=#EXTINF|\Z)'.format(re.escape(channel_name))
            match = re.search(channel_pattern, main_content, re.IGNORECASE | re.DOTALL)
            
            if not match:
                print(f"‚ùå Channel '{channel_name}' not found in main playlist")
                return main_content
            
            extinf_line = match.group(1)
            existing_content = match.group(2)
            full_block = match.group(0)
            
            # Existing URLs extract ‡¶ï‡¶∞‡ßÅ‡¶®
            existing_urls = extract_existing_urls(existing_content)
            print(f"   Existing URLs in main playlist: {len(existing_urls)}")
            
            # ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü ‡¶´‡¶ø‡¶≤‡ßç‡¶ü‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
            unique_new_urls = []
            duplicate_urls = []
            
            for url in new_urls:
                if url in existing_urls:
                    duplicate_urls.append(url)
                    print(f"   ‚ö†Ô∏è Duplicate found (skipping): {url}")
                else:
                    unique_new_urls.append(url)
            
            print(f"   Unique new URLs to add: {len(unique_new_urls)}")
            print(f"   Duplicate URLs skipped: {len(duplicate_urls)}")
            
            # Check if AUTO-UPDATED section already exists
            auto_updated_pattern = r'(#AUTO-UPDATED[^\n]*\n)((?:http[^\n]*\n)*)'
            auto_match = re.search(auto_updated_pattern, existing_content)
            
            if unique_new_urls:
                # ‡¶®‡¶§‡ßÅ‡¶® URLs ‡¶•‡ßá‡¶ï‡ßá AUTO-UPDATED section ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
                auto_urls_text = "".join([f"{url}\n" for url in unique_new_urls])
                
                if len(unique_new_urls) > 1:
                    auto_header = f"#AUTO-UPDATED - Multiple Channel - {timestamp} üîÑüîÑ\n"
                else:
                    auto_header = f"#AUTO-UPDATED - Channel - {timestamp} üîÑ\n"
                
                auto_section = auto_header + auto_urls_text
                
                if auto_match:
                    # AUTO-UPDATED section exists
                    if is_manual_run:
                        # Manual run: ‡¶®‡¶§‡ßÅ‡¶® URLs ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶® existing AUTO-UPDATED section ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞‡ßá
                        old_auto_section = auto_match.group(0)
                        # ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶®‡¶§‡ßÅ‡¶® URLs ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®, ‡¶™‡ßÅ‡¶∞‡¶æ‡¶§‡¶® AUTO-UPDATED URLs ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®
                        updated_auto_section = auto_section + old_auto_section
                        updated_content = existing_content.replace(old_auto_section, updated_auto_section)
                        print(f"‚úÖ Manual run: Added {len(unique_new_urls)} new URLs to AUTO-UPDATED section")
                    else:
                        # Scheduled run: ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶®‡¶§‡ßÅ‡¶® URLs ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶® existing AUTO-UPDATED section ‡¶è
                        old_auto_header = auto_match.group(1)
                        old_auto_urls = auto_match.group(2)
                        
                        # Combine old and new URLs, remove duplicates
                        all_urls = set()
                        # ‡¶™‡ßÅ‡¶∞‡¶æ‡¶§‡¶® URLs ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
                        for old_url in old_auto_urls.strip().split('\n'):
                            if old_url and old_url.startswith('http'):
                                all_urls.add(old_url)
                        # ‡¶®‡¶§‡ßÅ‡¶® URLs ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
                        for new_url in unique_new_urls:
                            all_urls.add(new_url)
                        
                        # ‡¶∏‡¶æ‡¶ú‡¶æ‡¶®‡ßã URLs ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
                        sorted_urls = sorted(list(all_urls))
                        new_auto_urls_text = "".join([f"{url}\n" for url in sorted_urls])
                        
                        # ‡¶®‡¶§‡ßÅ‡¶® header ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶® (timestamp update ‡¶ï‡¶∞‡ßÅ‡¶®)
                        if len(sorted_urls) > 1:
                            new_auto_header = f"#AUTO-UPDATED - Multiple Channel - {timestamp} üîÑüîÑ\n"
                        else:
                            new_auto_header = f"#AUTO-UPDATED - Channel - {timestamp} üîÑ\n"
                        
                        new_auto_section = new_auto_header + new_auto_urls_text
                        updated_content = existing_content.replace(old_auto_section, new_auto_section)
                        print(f"‚úÖ Scheduled run: Updated AUTO-UPDATED section with {len(sorted_urls)} URLs")
                else:
                    # No AUTO-UPDATED section exists - manual run
                    updated_content = auto_section + existing_content
                    print(f"‚úÖ Manual run: Added AUTO-UPDATED section with {len(unique_new_urls)} new URLs")
                
                # ‡¶°‡ßÅ‡¶™‡ßç‡¶≤‡¶ø‡¶ï‡ßá‡¶ü URLs ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
                if duplicate_urls:
                    duplicate_comment = f"# Already Have These Channel Links, No Need To Update - {timestamp}\n"
                    for dup_url in duplicate_urls:
                        duplicate_comment += f"# {dup_url}\n"
                    updated_content = duplicate_comment + updated_content
            else:
                # No new unique URLs found
                updated_content = existing_content
                if duplicate_urls:
                    duplicate_comment = f"# Already Have These Channel Links, No Need To Update - {timestamp}\n"
                    for dup_url in duplicate_urls:
                        duplicate_comment += f"# {dup_url}\n"
                    
                    if auto_match:
                        # AUTO-UPDATED section ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞‡ßá ‡¶ï‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
                        old_auto_section = auto_match.group(0)
                        updated_content = existing_content.replace(old_auto_section, duplicate_comment + old_auto_section)
                    else:
                        # ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶ï‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßÅ‡¶®
                        updated_content = duplicate_comment + existing_content
                
                print(f"‚ÑπÔ∏è No new unique URLs to add for {channel_name}")
            
            # ‡¶®‡¶§‡ßÅ‡¶® ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤ ‡¶¨‡ßç‡¶≤‡¶ï ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®
            new_channel_block = extinf_line + updated_content
            
            # ‡¶™‡ßÅ‡¶∞‡ßã ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü‡ßá replace ‡¶ï‡¶∞‡ßÅ‡¶®
            updated_main_content = main_content.replace(full_block, new_channel_block)
            
            return updated_main_content
        
        # ‡¶Æ‡ßá‡¶á‡¶® ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('main_playlist.m3u', 'r', encoding='utf-8') as f:
            main_content = f.read()
        
        updated_content = main_content
        total_channels_updated = 0
        total_links_added = 0
        total_duplicates_skipped = 0
        
        # ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®
        for main_channel, search_keywords in CHANNEL_MAPPING.items():
            print(f"\nüîç Processing: {main_channel}")
            print(f"   Searching for exact matches: {search_keywords}")
            
            # ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡ßá‡¶≤‡¶ø‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá URLs ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®
            channel_urls = find_channel_urls('source_playlist.m3u', search_keywords)
            
            if channel_urls:
                original_content = updated_content
                updated_content = update_channel_in_playlist(updated_content, main_channel, channel_urls)
                
                # Check if any changes were made
                if original_content != updated_content:
                    total_channels_updated += 1
                    # Count how many new URLs were actually added
                    existing_urls_before = extract_existing_urls(original_content)
                    existing_urls_after = extract_existing_urls(updated_content)
                    new_urls_count = len(existing_urls_after - existing_urls_before)
                    total_links_added += new_urls_count
            else:
                print(f"‚ö†Ô∏è Skipping {main_channel} - no URLs found")
        
        # ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶ï‡¶®‡ßç‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßÅ‡¶®
        with open('sirometv_updated.m3u', 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        print(f"\nüéâ Update Summary:")
        print(f"   Run type: {'Manual' if os.environ.get('GITHUB_EVENT_NAME') == 'workflow_dispatch' else 'Scheduled'}")
        print(f"   Total channels processed: {len(CHANNEL_MAPPING)}")
        print(f"   Total channels updated: {total_channels_updated}")
        print(f"   Total new links added: {total_links_added}")
        print(f"   Total duplicates skipped: {total_duplicates_skipped}")
        
        EOF
        
    - name: Verify and replace playlist
      run: |
        echo "üîç Verifying updated playlist..."
        
        if [ ! -f "sirometv_updated.m3u" ] || [ ! -s "sirometv_updated.m3u" ]; then
          echo "‚ùå ERROR: Updated playlist is missing or empty!"
          exit 1
        fi
        
        # ‡¶Æ‡ßÇ‡¶≤ playlist backup ‡¶ï‡¶∞‡ßÅ‡¶®
        cp sirometv.m3u sirometv_backup.m3u 2>/dev/null || true
        
        # ‡¶Æ‡ßÇ‡¶≤ playlist ‡¶è‡¶∞ line count check ‡¶ï‡¶∞‡ßÅ‡¶®
        OLD_LINES=$(wc -l < sirometv.m3u)
        NEW_LINES=$(wc -l < sirometv_updated.m3u)
        echo "üìä Line count - Old: $OLD_LINES, New: $NEW_LINES"
        
        mv sirometv_updated.m3u sirometv.m3u
        
        echo "üîç Updated channels preview:"
        grep -B1 -A20 "AUTO-UPDATED" sirometv.m3u | head -60 || echo "No AUTO-UPDATED sections found"
        
        echo "üîç Duplicate comments preview:"
        grep -A5 "Already Have These Channel Links" sirometv.m3u | head -30 || echo "No duplicate comments found"
        
    - name: Commit and push changes
      run: |
        echo "üíæ Saving changes to repository..."
        
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git add sirometv.m3u
        
        if git diff --staged --quiet; then
          echo "‚úÖ No changes detected - playlist is up to date"
        else
          BANGLADESH_TIME=$(TZ='Asia/Dhaka' date +'%Y-%m-%d %I:%M:%S%p BD Time')
          
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            git commit -m "üîÑ Manual Update: Channel Links with Duplicate Check - $BANGLADESH_TIME"
          else
            git commit -m "üîÑ Scheduled Update: Channel Links with Duplicate Check - $BANGLADESH_TIME"
          fi
          git push
          echo "üéâ Successfully updated channel links with duplicate protection!"
        fi
        
    - name: Cleanup
      run: |
        rm -f main_playlist.m3u source_playlist.m3u sirometv_backup.m3u
        echo "‚úÖ Cleanup completed!"
